# -*- coding: utf-8 -*-
"""AKINSANYA BARAKAT VERTIBI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nUW-Fy4ufrNivpKc5TsZb0jsvcPtyCEs
"""

import numpy as np
import tensorflow as tf
import pandas as pd

from google.colab import files
uploaded = files.upload()

# import csv

# with open('tag_logit_per_word.tsv') as tsvfile:
#   emission = csv.reader(tsvfile, dialect='excel-tab')
#   for i in emission:
#     emm=i
#     print(emm)
df = pd.read_table('tag_logit_per_word.tsv')
df.columns
v=[j for i in np.array(df.drop(df.columns[0], axis=1)) for j in i]
len(v)

data = pd.read_csv('train_pos.txt', sep=" ", header=None)
priors=dict()
tag_prior=dict()
trans_prob=dict()

total_word_count = 0
total_word_count += len(data[0])

for i in range(len(data[0])):
  word = data[0][i]
  tag = data[1][i]
  if word in priors:
    priors[word]+=1
  else:
    priors[word]=1
  if tag in tag_prior:
    tag_prior[tag]+=1
  else:
    tag_prior[tag]=1
    
  if i > 0:
    prev_pos = data[1][i - 1]
    if prev_pos not in trans_prob:
      trans_prob[prev_pos] = {tag: 1}
    elif tag not in trans_prob[prev_pos]:
      trans_prob[prev_pos][tag] = 1
    else:
      trans_prob[prev_pos][tag] += 1
for word in priors:
  priors[word] = float(priors[word]) / float(total_word_count)
for pos in tag_prior:
  tag_prior[tag] = float(tag_prior[tag]) / float(total_word_count)
for prev_pos in trans_prob:
  total = 0
  for tag in trans_prob[prev_pos]:
    total += trans_prob[prev_pos][tag]
  for tag in trans_prob[prev_pos]:
    trans_prob[prev_pos][tag] = \
                    float(trans_prob[prev_pos][tag]) / float(total)

Trans=([list(i.values()) for i in trans_prob.values()])
#tag=([list(i.values()) for i in tag_prior.values()])
tag=[list(tag_prior.values())]

print(Trans)
print(tag)

class Vertibialgo():
 

    def __init__(self, Trans, Emm, prior, epsilon = 0.01, maxStep = 10):
        
 
                self.maxStep = maxStep
                self.epsilon = epsilon 
                self.S = len(Trans)
                self.O = len(Emm)
                self.prob_state_1 = []
                self.Emm = tf.Variable(Emm, dtype=tf.float64)
                #self.Trans = tf.Variable(Trans, dtype=tf.float64)
                self.prior = tf.Variable(tf.constant(prior, dtype=tf.float64))
    
    
    def beliefnet(self, scores):
        
        scores_reshape = tf.reshape(scores, (-1,1))
        return tf.add(scores_reshape, tf.log(self.T))
    
    def viterbi_inference(self, obs_seq):
   
        self.N = len(obs_seq)

        shape = [self.N, self.S]

        x = tf.constant(obs_seq, dtype=tf.int32)
        pathStates = tf.Variable(tf.zeros(shape, dtype=tf.int64))
        pathScores = tf.Variable(tf.zeros(shape, dtype=tf.float64))
        states_seq = tf.Variable(tf.zeros([shape[0]], dtype=tf.int64))        
        obs_prob_seq = tf.log(tf.gather(self.Emm, x))
        obs_prob_list = tf.split(obs_prob_seq, self.N, 0)

        with tf.name_scope('log-priors'):
            pathScores = tf.scatter_update(pathScores, 0, tf.log(self.prior) + tf.squeeze(obs_prob_list[0]))
            
        with tf.name_scope('Beliefnet'):
            for step, obs_prob in enumerate(obs_prob_list[1:]):

                with tf.name_scope('step' %step):
                    
                    belief = self.beliefnet(pathScores[step, :])
 
                    pathStates = tf.scatter_update(pathStates, step + 1, tf.argmax(belief, 0))
                    pathScores = tf.scatter_update(pathScores, step + 1, tf.reduce_max(belief, 0) + tf.squeeze(obs_prob))

            with tf.name_scope('Max_Likelihood'):
                states_seq = tf.scatter_update(states_seq, self.N-1, tf.argmax(pathScores[self.N-1, :], 0))
        
        with tf.name_scope():
            for step in range(self.N - 1, 0, -1):
                with tf.name_scope('Backsteps' %step):
                    state = states_seq[step]
                    idx = tf.reshape(tf.stack([step, state]), [1, -1])
                    state_prob = tf.gather_nd(pathStates, idx)
                    states_seq = tf.scatter_update(states_seq, step - 1,  state_prob[0])

        return states_seq, tf.exp(pathScores) 
    
    def implement(self, obs_seq, summary=False):
        
        state_graph, state_prob_graph = self.viterbi_inference(obs_seq)
        
        with tf.Session() as sess:
            
            sess.run(tf.global_variables_initializer())
            states_seq, state_prob = sess.run([state_graph, state_prob_graph])
            
            if summary:
                summary_writer = tf.summary.FileWriter('logs/', graph=sess.graph)

        return states_seq, state_prob



model=Vertibialgo(Trans,v,tag)